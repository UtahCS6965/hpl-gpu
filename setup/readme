#
# - Compile time options -----------------------------------------------
#
# -DHPL_COPY_L           force the copy of the panel L before bcast;
# -DHPL_NO_MPI_DATATYPE  Do not use custom MPI types
# -DHPL_DETAILED_TIMING  enable detailed timers;
# -DHPL_CALL_CALDGEMM    use caldgemm for DGEMM cals
# -DTRACE_CALLS          function level tracing for calls that might
#                        be relevant for optimization (implies HPL_GPU_NOT_QUIET)
# -DUSE_ORIGINAL_LASWP   use original laswp implementation
# -DTRACE_PERMDATA       dump LINDXA(U) in laswp0[16]T
# -DHPL_FASTINIT         Fast initialization of input matrices for tuning runs
# -DHPL_FASTVERIFY       Use Fast initialization random number generator for verification
# -DHPL_PAGELOCKED_MEM   Allocate the memory pagelocked
# -DHPL_HUGE_TABLES      Allocate the memory using huge tables
# -DHPL_GPU_TIMING       Force Display of CALDGEMM Timing Data without HPL_GPU_NOT_QUIET
# -DHPL_GPU_NOT_QUIET    Do not set quiet parameter for CALDGEMM (Will also display timing)
# -DHPL_GPU_PERFORMANCE_WARNINGS
#                        Print performance warnings for suboptimal CALDGEMM execution
# -DHPL_SEND_U_PADDING   Transmit the padding of U matrix, unsafe if transfering only parts of U
# -DHPL_GPU_VERIFY       Verify result of caldgemm calls
# -DCALDGEMM_TEST        Activate Test Debug Code
# -DHPL_PRINT_INTERMEDIATE
#                        print intermediate performance results
# -DHPL_PRINT_AVG_MATRIX_SIZE
#                        show how much memory the matrix uses per node on average
# -DHPL_MPI_FUNNELED_THREADING
#                        Do all MPI calls from main thread, otherwise MPI lib must be thread safe
# -DHPL_PRINT_THROTTLING_NODES=<GPU CLOCK> set the gpu clock so caldgemm knows which reference to compare with
# -DHPL_NO_MPI_THREAD_CHECK
#                        HPL will not check whether the MPI lib has sufficiant threading capabilities but just call MPI_Init
# -DHPL_START_PERCENTAGE=<float>
#                        Approximate Percentage of the runtime where to start factorization by skipping cols in the matrix. As one cannot start at an arbitrary N easily this is not exact.
# -DHPL_NO_HACKED_LIB    Do not use the hacked ATI lib
# -DHPL_HAVE_PREFETCHW   AMD CPUs have a prefetchw instruction which makes some prefetches more efficient.
# -DHPL_NO_MPI_LIB       No MPI, one single node run possible.
# -DHPL_GPU_MAX_NB       Set max NB for GPU HPL (default 1024)
# -DHPL_SLOW_CPU         Use special code paths optimized for slow CPUs and a GPU
# -DHPL_FAST_GPU         Similar to slowCPU, better suited for medium CPU and fast GPU
# -DHPL_RESTRICT_CPUS=   Restrict CPU threads used for factorization with lookahead, set to NO (0), YES (1), DYNAMIC (2), or DYNAMIC WITH CUT (3).
# -DHPL_MULTI_GPU        Use multiple GPUs
# -DHPL_GPU_MAPPING="{x,y,z}"
#                        Map gpu 0 to core x, 1 to y and so on
# -DHPL_GPU_POSTPROCESS_MAPPING="C{x,y,z}"
#                        Map gpu postprocess 0 to core x, 1 to y and so on
# -DHPL_GPU_ALLOC_MAPPING="{x,y,z}"
#                        Same as above for allocation
# -DHPL_GPU_DMA_MAPPING="{x,y,z}"
#                        Same as above for DMA
# -DHPL_GPU_EXCLUDE_CORES="{x,y}"
#                        Exclude CPU cores x,y from processing completely
# -DHPL_GPU_DEVICE_IDS="{x,y}"
#                        Device IDs of GPUs to use
# -DHPL_GPU_PIN_MAIN=i   CPU core for main thread
# -DHPL_DISABLE_LOOKAHEAD=n
#                        Disable lookahead algorithm as soon as trailing matrix size hits n
# -DHPL_LOOKAHEAD2_TURNOFF=n
#                        Same for lookahead2
# -DHPL_HALF_BLOCKING=n  n at which only half the blocking size is used
# -DHPL_GPU_FACTORIZE    Use GPU for factorization, requires ACML-GPU to be installed
# -DHPL_USE_ALL_CORES_FOR_LASWP
#                        Use all cores for LASWP, lookahead 2 will not work well this way
# -DHPL_GPU_THREADSAVE_DRIVER
#                        Assume GPU driver to be threadsave
# -DHPL_CUSTOM_PARAMETER_CHANGE
#                        Custom code executed every iteration, good for changing factorization parameters
# -DHPL_GPU_EXTRA_CALDGEMM_OPTIONS
#                        Extra source code that defines caldgemm options e.g. "cal_info.OutputThreads = 2;"
# -DHPL_CALDGEMM_CBLAS_WRAPPER
#                        Wrap CBLAS calls through CALDGEMM reducing the number of threads for small BLAS operations for better performance (only for OpenMP based BLAS libraries)
# -DHPL_INTERLEAVE_MEMORY
#                        Interleave memory between NUMA nodes
# -DHPL_EMULATE_MULTINODE
#                        Report a multi node run to caldgemm and provide a fake function for the broadcast callback to simulate the loss due to communication overhead
# -DHPL_PAUSE=n          Sleep during the iterations for n * NRest  micro-seconds (NRest is the remaining matrix size). Usefull if the hardware overheats. Timers are stopped during this time. Clearly not valid for official results.
# -DHPL_ALTERNATE_LOOKAHEAD=n
#                        Use caldgemm AlternateLookahead feature with setting n
# -DHPL_CALDGEMM_BACKEND=x
#                        Use the CALDGEMM backend x, default: cal
# -DHPL_LOOKAHEAD_2B     Enable lookahead mode 2b
# -DHPL_LOOKAHEAD_2B_FIXED_STEPSIZE=n
#                        Use a fixed starting stepsize for lookahead 2b instead of doing an mpi_allreduce to determine the minimum stepsize
# -DHPL_GPU_TEMPERATURE_THRESHOLD=temp
#                        Temperature threshold where the run is stopped
# -DHPL_MPI_WRAPPERS     Use MPI Wrappers to limit Max MPI Message size. Message size can be controlled via HPL_MAX_MPI_SEND_SIZE and HPL_MAX_MPI_BCAST_SIZE
# -DHPL_COPYL_DURING_FACT
#                        Perform copyL (if necessary) during factorization to allow for multithreading
# -DHPL_ASYNC_DLATCPY    Perform DLATCPY asynchronously during DGEMM execution

#
# By default HPL will:
#    *) not copy L before broadcast,
#    *) use custom MPI types
#    *) not display detailed timing information.
#    *) not use CALDGEMM
#    *) not trace calls
#    *) use improved LASWP
#    *) not trace LASWP arrays
#    *) use original initialization and verification
#    *) not use pagelocked memory
#    *) not use huge tables
#    *) set GPU to quiet mode
#    *) not print performance warnings
#    *) not sent U padding
#    *) not verify CALDGEMM results
#    *) not print intermediate results nor matrix size
#    *) use serialized MPI threading
#    *) do not print throttling nodes
#    *) break if MPI library does not provide the required threading support
#    *) use the hacked AMD driver
#    *) not use PREFETCHW instruction
#    *) use MPI
#    *) max GPU NB is 1024
#    *) not set slow CPU
#    *) not set fast GPU
#    *) restrict CPUs dynamically
#    *) use the first GPU in the system only
#    *) map all GPUs to core 0
#    *) map GPU main thread to core 0
#    *) never disable lookahead 1/2
#    *) not use GPU for factorization