#
# - Compile time options -----------------------------------------------
#
# -DHPL_COPY_L           force the copy of the panel L before bcast;
# -DHPL_NO_MPI_DATATYPE  Do not use custom MPI types
# -DHPL_DETAILED_TIMING  enable detailed timers;
# -DHPL_CALL_CALDGEMM    use caldgemm for DGEMM cals
# -DTRACE_CALLS          function level tracing for calls that might
#                        be relevant for optimization (implies HPL_GPU_NOT_QUIET)
# -DUSE_ORIGINAL_LASWP   use original laswp implementation
# -DTRACE_PERMDATA       dump LINDXA(U) in laswp0[16]T
# -DHPL_FASTINIT         Fast initialization of input matrices for tuning runs
# -DHPL_FASTVERIFY       Use Fast initialization random number generator for verification
# -DHPL_PAGELOCKED_MEM   Allocate the memory pagelocked
# -DHPL_HUGE_TABLES      Allocate the memory using huge tables
# -DHPL_GPU_TIMING       Force Display of CALDGEMM Timing Data without HPL_GPU_NOT_QUIET
# -DHPL_GPU_NOT_QUIET    Do not set quiet parameter for CALDGEMM (Will also display timing)
# -DHPL_GPU_PERFORMANCE_WARNINGS
#                        Print performance warnings for suboptimal CALDGEMM execution
# -DHPL_SEND_U_PADDING   Transmit the padding of U matrix, unsafe if transfering only parts of U
# -DHPL_GPU_VERIFY       Verify result of caldgemm calls
# -DCALDGEMM_TEST        Activate Test Debug Code
# -DHPL_PRINT_INTERMEDIATE
#                        print intermediate performance results
# -DHPL_PRINT_AVG_MATRIX_SIZE
#                        show how much memory the matrix uses per node on average
# -DHPL_MPI_FUNNELED_THREADING
#                        Do all MPI calls from main thread, otherwise MPI lib must be thread safe
# -DHPL_PRINT_THROTTLING_NODES=<GPU CLOCK> set the gpu clock so caldgemm knows which reference to compare with
# -DHPL_NO_MPI_THREAD_CHECK
#                        HPL will not check whether the MPI lib has sufficiant threading capabilities but just call MPI_Init
# -DHPL_START_PERCENTAGE=<float>
#                        Approximate Percentage of the runtime where to start factorization by skipping cols in the matrix. As one cannot start at an arbitrary N easily this is not exact.
# -DHPL_NO_HACKED_LIB    Do not use the hacked ATI lib
# -DHPL_HAVE_PREFETCHW   AMD CPUs have a prefetchw instruction which makes some prefetches more efficient.
# -DHPL_NO_MPI_LIB       No MPI, one single node run possible.
# -DHPL_GPU_MAX_NB       Set max NB for GPU HPL (default 1024)
# -DHPL_SLOW_CPU         Use special code paths optimized for slow CPUs and a GPU
# -DHPL_FAST_GPU         Similar to slowCPU, better suited for medium CPU and fast GPU
# -DHPL_RESTRICT_CPUS=   Restrict CPU threads used for factorization with lookahead, set to NO (0), YES (1), DYNAMIC (2), or DYNAMIC WITH CUT (3).
# -DHPL_MULTI_GPU        Use multiple GPUs
# -DHPL_GPU_MAPPING={x,y,z} Map gpu 0 to core x, 1 to y and so on
# -DHPL_GPU_POSTPROCESS_MAPPING={x,y,z} Map gpu postprocess 0 to core x, 1 to y and so on
# -DHPL_GPU_ALLOCS_MAPPING={x,y,z} Same as above for allocation
# -DHPL_GPU_PIN_MAIN=i   CPU core for main thread
# -DHPL_DISABLE_LOOKAHEAD=n
#                        Disable lookahead algorithm as soon as trailing matrix size hits n
# -DHPL_LOOKAHEAD2_TURNOFF=n
#                        Same for lookahead2
# -DHPL_GPU_FACTORIZE    Use GPU for factorization, requires ACML-GPU to be installed
# -DHPL_USE_ALL_CORES_FOR_LASWP
#                        Use all cores for LASWP, lookahead 2 will not work well this way
# -DHPL_GPU_THREADSAVE_DRIVER
#                        Assume GPU driver to be threadsave
#
# By default HPL will:
#    *) not copy L before broadcast,
#    *) use custom MPI types
#    *) not display detailed timing information.
#    *) not use CALDGEMM
#    *) not trace calls
#    *) use improved LASWP
#    *) not trace LASWP arrays
#    *) use original initialization and verification
#    *) not use pagelocked memory
#    *) not use huge tables
#    *) set GPU to quiet mode
#    *) not print performance warnings
#    *) not sent U padding
#    *) not verify CALDGEMM results
#    *) not print intermediate results nor matrix size
#    *) use serialized MPI threading
#    *) do not print throttling nodes
#    *) break if MPI library does not provide the required threading support
#    *) use the hacked AMD driver
#    *) not use PREFETCHW instruction
#    *) use MPI
#    *) max GPU NB is 1024
#    *) not set slow CPU
#    *) not set fast GPU
#    *) restrict CPUs dynamically
#    *) use the first GPU in the system only
#    *) map all GPUs to core 0
#    *) map GPU main thread to core 0
#    *) never disable lookahead 1/2
#    *) not use GPU for factorization