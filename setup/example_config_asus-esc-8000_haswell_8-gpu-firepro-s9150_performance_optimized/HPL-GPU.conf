#############################################################################################################
# HPL-GPU 2.0 Runtime configuration file
#
# This is a sample file to demonstrate how to run HPL-GPU in performance-optimized mode on ASUS ESC8000.
# The setup is a dual-socket 10 core CPU configuration with Hyperthreading. Cores 0-9 belong to socket 0,
# cores 10-19 belong to socket 1, cores 20-29 are the virtual Hyperthreading cores of socket 0, and cores 30-
# 39 the Hyperthreading cores of socket 1. The system has 8 GPUs: GPUs 0 to 3 connected to CPU socket 0 and
# GPUs 4 to 7 connected to socket 1. This configuration will result in the following setup:
# - GPU usage is interleaved among NUMA nodes and then between the PCIe switches for best performance
# - GPU thread runs on core 10, main thread on core 0, to use both CPU sockets.
# - BLAS and LASWP uses cores 0-9 and 10-19, so always 1 virtual core of each physical core except the GPU one
# - MPI and broadcast threads are placed on core 30, using the 2nd virtual core of the GPU thread.
# - NB varies between 960 and 4320 going down step by step at matrix sizes indicated by the threshold.
#   This ensures max dgemm performance for most of the time (nb = 4320) and ensures the cpu is not bottleneck
# - We always use the alternate lookahead, and we always run the entire update-dgemm on the GPU.
#   The CPU contribution is so small, that putting dgemm workload onto the CPU has a negative total effect.
#   However, we can run some of the dgemm inside the factorization on the CPU. We change this threshold
#   gradually in Make.Generic.Config. This, together with the update-dtrsm almost fully loads the CPU.
# - We use an AMD GPU so we set -OQ
# - We use lookahead 3 with pipelined mode. Since the GPUs have enough memory, we use -Ab 1
# - We do not need lookahead turnoff
# - ASYNC_FACT_DGEMM is always used with varying -Or setting to increase CPU utilization.
# - ASNYC_DTRSM is used for n < 215000 (experimentally determined)
#############################################################################################################

#Custom CALDGEMM Parameters:
HPL_PARAMDEFS:

#In case the selected backend is OpenCL, this setting chooses the 3rd party library file that provides the OpenCL kernels.
HPL_PARAMDEFS: -Ol amddgemm.so

#This sets the GPU ratio, i.e. the fraction of the workload performed by the GPU. The following options exist: 1.0: All work performed by GPU, 0.0 <= GPURatio < 1.0: Fixed setting of
#GPU Ratio, -1.0: Fully automatic GPU Ratio calculation, -1.0 < GPURatio < 0.0: This defines the negative initial GPU Ratio: during runtime the ratio is then dynamically adapted. This
#means e.g. GPURatio = -0.9 will start with a ratio of 0.9, and then adapt it dynamically. The suggested settings are: 1.0 for GPU only and for efficiency optimized cases; In any other
#case a negative initial setting is suggested. Please see also the advence GPURatio options below.
HPL_PARAMDEFS: -j 1.0

#This defines the CPU core used for the management thread (-K) and for the broadcast thread (-Kb: -2 = MPI affinity). If unset this is autodetected. Optionally set -KG and -KN
HPL_PARAMDEFS: -K 10 -Kb -2 				//K = 1 * number of physical cores per socket (= 1st virtual core of 1st core on socket 2)
HPL_PARAMDEFS: -KG 0 -KN 20

#Local matrix size for which alternate lookahead is activated. Usualy you want to leave this infinitely high. For optimized efficiency, always activate.
#For optimized performance, you can lower this setting in some cases. Optimal value must be tuned manually.
HPL_PARAMDEFS: -Ca 10000000

#Use the fast random number generator (faster initialization of the program, should be disabled for official runs)
#0: Disabled, 1: Only fast initialization (cannot verify), 2: Fast Initialization and Verification (default)
#HPL_FASTRAND: 2

#You can set several thresholds. If the remaining global matrix dimension is above the n-th threshold, the current NB for the next iteration is multiplied by the n-th multiplier
HPL_NB_MULTIPLIER_THRESHOLD: 106000;94000;76000;66000;35000;28000;1
HPL_NB_MULTIPLIER: 9;7;6;5;4;3;2

#############################################################################################################
#All the following are optional tuning options
#############################################################################################################

#HPL-GPU can pass the command line parameters of dgemm_bench to caldgemm, these parameters are evaluated last overwriting all other settings, in this example restricting CALDGEMM to 2 GPUs.
HPL_PARAMDEFS: -Y 8 -Yu

#Pin MPI runtime threads to these CPU core(s)
HPL_MPI_AFFINITY: 30 					//3 * number of physical cores per socket (= 2nd virtual core of 1st core on socket 2

#Change the thread order for NUMA architectures: 0: Default order, 1: Optimized order for CPU with Hyperthreading and interterleaved virtual cores, 2: Optimized for other NUMA systems.
#It might make sense to just try all settings
#HPL_PARAMDEFS: -: 0

#Mostly on Intel CPUs with Hyperthreading, it make make sense to limit the number of LASWP cores to the number of physical cores - n (with n the number of caldgemm threads, 1 for OpenCL)
HPL_NUM_LASWP_CORES: 19

#NUMA Memory interleaving: 0 -> Disabled, 1 -> Interleave all memory, 2 -> Interleave matrix-memory (default)
#HPL_INTERLEAVE_MEMORY: 2

#You can reorder the GPU device numbering. In general, it is good to interleave NUMA nodes, i.e. if you have 2 NUMA nodes, 4 GPUs, GPUs 0 and 1 on node 0, GPUs 2 and 3 on node 1, the
#below setting is suggested. Keep in mind that the altered numbering affects other settings relative to GPU numbering, e.g. GPU_ALLOC_MAPPING
HPL_PARAMDEFS: -/ 0;4;2;6;1;5;3;7 

#Define CPU cores used to allocated GPU related memory. You should chose the correct NUMA nodes. For the above 4-GPU example and 2 10-core CPUs the following would be good:
HPL_PARAMDEFS: -UA0 0 -UA1 39 -UA2 0 -UA3 39 -UA4 0 -UA5 39 -UA6 0 -UA7 39

#SimpleGPUScheduling mode: 0 / outcommented: standard round-robin use of command queues - 1 / use dedicated queues for sent, receive, kernels.
HPL_PARAMDEFS: -OQ

#Outcomment the following to support lookahead 3 and set -Aq appropriate
HPL_PARAMDEFS: -Ap -Aq 25000 -Ab 1

#Tool to find the duration of the core phase of HPL, needed to measure power consumption and power efficiency.
#HPL_DURATION_FIND_HELPER

#Offload some steps during the factorization to the GPU. Offloading is active as soon as local matrix size is lower than the defined parameter.
#For best efficiency, you want to enable this always, i.e. set the parameter very high. For best performance, parameter must be tuned manually.
HPL_CALDGEMM_ASYNC_FACT_DGEMM: 10000000
HPL_CALDGEMM_ASYNC_FACT_FIRST
HPL_PARAMDEFS: -Oa
#HPL_PARAMDEFS: -Or 480 				//AsyncDGEMMThreshold Handled by HPL_CUSTOM_PARAMETER_CHANGE_CALDGEMM

#Same as above, but for the large Update-DTRSM (and DTRSM inside the factorization below). Optimal value for performance and efficiency is usually identical. Value must be tuned manually.
#(You can use HPL_CALDGEMM_ASYNC_DTRSM_MIN_NB in combination with HPL_NB_MULTIPLIER to disable the async dtrsm below a certain NB).
HPL_CALDGEMM_ASYNC_DTRSM: 215000
HPL_PARAMDEFS: -Od
HPL_CALDGEMM_ASYNC_FACT_DTRSM: 0
HPL_CALDGEMM_ASYNC_DTRSM_MIN_NB: 480

#Preallocate some data to avoid unnecessary dynamic mallocs, define number of BBuffers to save memory. PreallocData should be larger than local matrix size / Nb. max_bbuffers should be larger than local matrix size / (Number of GPUs * Nb).
HPL_PARAMDEFS: -Op 70 -bb 15

#Enable (default) / disable HPL warmup iteration
HPL_WARMUP
