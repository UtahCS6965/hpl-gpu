#############################################################################################################
# HPL-GPU 1.2 Runtime configuration file
#
# This is a variant of the configuration in the parent folder. The difference is that the configuration in
# the parent folder is for systems with Hyperthreading and this file is for systems without.
# The difference is essntially the pinning of the broadcast thread, which no longer takes a second virtual
# core. In addition, GPU allocation pinnings are changed from 0/39 to 0/19
#############################################################################################################

#Custom CALDGEMM Parameters:
HPL_PARAMDEFS:

#In case the selected backend is OpenCL, this setting chooses the 3rd party library file that provides the OpenCL kernels.
HPL_PARAMDEFS: -Ol amddgemm.so

#This sets the GPU ratio, i.e. the fraction of the workload performed by the GPU. The following options exist: 1.0: All work performed by GPU, 0.0 <= GPURatio < 1.0: Fixed setting of
#GPU Ratio, -1.0: Fully automatic GPU Ratio calculation, -1.0 < GPURatio < 0.0: This defines the negative initial GPU Ratio: during runtime the ratio is then dynamically adapted. This
#means e.g. GPURatio = -0.9 will start with a ratio of 0.9, and then adapt it dynamically. The suggested settings are: 1.0 for GPU only and for efficiency optimized cases; In any other
#case a negative initial setting is suggested. Please see also the advence GPURatio options below.
HPL_PARAMDEFS: -j 1.0

#This defines the CPU core used for the management thread (-K) and for the broadcast thread (-Kb: -2 = MPI affinity). If unset this is autodetected. Optionally set -KG and -KN
HPL_PARAMDEFS: -K 10 -Kb -2 				//K = 1 * number of physical cores per socket (= 1st virtual core of 1st core on socket 2)
HPL_PARAMDEFS: -KG 0 -KN 20

#Local matrix size for which alternate lookahead is activated. Usualy you want to leave this infinitely high. For optimized efficiency, always activate.
#For optimized performance, you can lower this setting in some cases. Optimal value must be tuned manually.
HPL_PARAMDEFS: -Ca 10000000

#Use the fast random number generator (faster initialization of the program, should be disabled for official runs)
#0: Disabled, 1: Only fast initialization (cannot verify), 2: Fast Initialization and Verification (default)
#HPL_FASTRAND: 2

#You can set several thresholds. If the remaining global matrix dimension is above the n-th threshold, the current NB for the next iteration is multiplied by the n-th multiplier
HPL_NB_MULTIPLIER_THRESHOLD: 106000;94000;76000;66000;35000;28000;1
HPL_NB_MULTIPLIER: 9;7;6;5;4;3;2

#############################################################################################################
#All the following are optional tuning options
#############################################################################################################

#HPL-GPU can pass the command line parameters of dgemm_bench to caldgemm, these parameters are evaluated last overwriting all other settings, in this example restricting CALDGEMM to 2 GPUs.
HPL_PARAMDEFS: -Y 8 -Yu

#Pin MPI runtime threads to these CPU core(s)
HPL_MPI_AFFINITY: 19 					//3 * number of physical cores per socket (= 2nd virtual core of 1st core on socket 2

#Change the thread order for NUMA architectures: 0: Default order, 1: Optimized order for CPU with Hyperthreading and interterleaved virtual cores, 2: Optimized for other NUMA systems.
#It might make sense to just try all settings
#HPL_PARAMDEFS: -: 0

#Mostly on Intel CPUs with Hyperthreading, it make make sense to limit the number of LASWP cores to the number of physical cores - n (with n the number of caldgemm threads, 1 for OpenCL)
HPL_NUM_LASWP_CORES: 19

#NUMA Memory interleaving: 0 -> Disabled, 1 -> Interleave all memory, 2 -> Interleave matrix-memory (default)
#HPL_INTERLEAVE_MEMORY: 2

#You can reorder the GPU device numbering. In general, it is good to interleave NUMA nodes, i.e. if you have 2 NUMA nodes, 4 GPUs, GPUs 0 and 1 on node 0, GPUs 2 and 3 on node 1, the
#below setting is suggested. Keep in mind that the altered numbering affects other settings relative to GPU numbering, e.g. GPU_ALLOC_MAPPING
HPL_PARAMDEFS: -/ 0;4;2;6;1;5;3;7 

#Define CPU cores used to allocated GPU related memory. You should chose the correct NUMA nodes. For the above 4-GPU example and 2 10-core CPUs the following would be good:
HPL_PARAMDEFS: -UA0 0 -UA1 19 -UA2 0 -UA3 19 -UA4 0 -UA5 19 -UA6 0 -UA7 19

#SimpleGPUScheduling mode: 0 / outcommented: standard round-robin use of command queues - 1 / use dedicated queues for sent, receive, kernels.
HPL_PARAMDEFS: -OQ

#Outcomment the following to support lookahead 3 and set -Aq appropriate
HPL_PARAMDEFS: -Ap -Aq 25000 -Ab 1

#Tool to find the duration of the core phase of HPL, needed to measure power consumption and power efficiency.
#HPL_DURATION_FIND_HELPER

#Offload some steps during the factorization to the GPU. Offloading is active as soon as local matrix size is lower than the defined parameter.
#For best efficiency, you want to enable this always, i.e. set the parameter very high. For best performance, parameter must be tuned manually.
HPL_CALDGEMM_ASYNC_FACT_DGEMM: 10000000
HPL_CALDGEMM_ASYNC_FACT_FIRST
HPL_PARAMDEFS: -Oa
#HPL_PARAMDEFS: -Or 480 				//AsyncDGEMMThreshold Handled by HPL_CUSTOM_PARAMETER_CHANGE_CALDGEMM

#Same as above, but for the large Update-DTRSM (and DTRSM inside the factorization below). Optimal value for performance and efficiency is usually identical. Value must be tuned manually.
#(You can use HPL_CALDGEMM_ASYNC_DTRSM_MIN_NB in combination with HPL_NB_MULTIPLIER to disable the async dtrsm below a certain NB).
HPL_CALDGEMM_ASYNC_DTRSM: 215000
HPL_PARAMDEFS: -Od
HPL_CALDGEMM_ASYNC_FACT_DTRSM: 0
HPL_CALDGEMM_ASYNC_DTRSM_MIN_NB: 480

#Preallocate some data to avoid unnecessary dynamic mallocs, define number of BBuffers to save memory. PreallocData should be larger than local matrix size / Nb. max_bbuffers should be larger than local matrix size / (Number of GPUs * Nb).
HPL_PARAMDEFS: -Op 70 -bb 15

#Enable (default) / disable HPL warmup iteration
HPL_WARMUP
